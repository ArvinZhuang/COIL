import randomimport datasetsfrom collections import defaultdictfrom typing import Union, List, Callable, Dictfrom torch.utils.data import Datasetfrom arguments import DataArgumentsfrom textattack.transformations import WordSwapNeighboringCharacterSwap, \    WordSwapRandomCharacterDeletion, WordSwapRandomCharacterInsertion, \    WordSwapRandomCharacterSubstitution, WordSwapQWERTYfrom textattack.augmentation import Augmenterfrom textattack.transformations import CompositeTransformationfrom textattack.constraints.pre_transformation import MinWordLength, StopwordModificationfrom transformers import PreTrainedTokenizer, BatchEncoding, EvalPredictionSTOPWORDS = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've",                 "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',                 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself',                 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',                 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',                 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',                 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',                 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',                 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',                 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',                 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',                 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don',                 "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',                 "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't",                 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',                 "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't",                 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"])class FixWordSwapQWERTY(WordSwapQWERTY):    def _get_replacement_words(self, word):        if len(word) <= 1:            return []        candidate_words = []        start_idx = 1 if self.skip_first_char else 0        end_idx = len(word) - (1 + self.skip_last_char)        if start_idx >= end_idx:            return []        if self.random_one:            i = random.randrange(start_idx, end_idx + 1)            if len(self._get_adjacent(word[i])) == 0:                candidate_word = (                        word[:i] + random.choice(list(self._keyboard_adjacency.keys())) + word[i + 1:]                )            else:                candidate_word = (                        word[:i] + random.choice(self._get_adjacent(word[i])) + word[i + 1:]                )            candidate_words.append(candidate_word)        else:            for i in range(start_idx, end_idx + 1):                for swap_key in self._get_adjacent(word[i]):                    candidate_word = word[:i] + swap_key + word[i + 1 :]                    candidate_words.append(candidate_word)        return candidate_wordsclass GroupedMarcoTrainDataset(Dataset):    query_columns = ['qid', 'query']    document_columns = ['pid', 'passage']    def __init__(            self,            args: DataArguments,            path_to_tsv: Union[List[str], str],            tokenizer: PreTrainedTokenizer,            cache_dir: str,    ):        self.nlp_dataset = datasets.load_dataset(            'json',            data_files=path_to_tsv,            ignore_verifications=False,            cache_dir=cache_dir,            features=datasets.Features({                'qry': {                    'qid': datasets.Value('string'),                    'query': [datasets.Value('int32')],                },                'pos': [{                    'pid': datasets.Value('string'),                    'passage': [datasets.Value('int32')],                }],                'neg': [{                    'pid': datasets.Value('string'),                    'passage': [datasets.Value('int32')],                }]}            )        )['train']        self.tok = tokenizer        self.flips = defaultdict(lambda: random.random() > 0.5)        self.SEP = [self.tok.sep_token_id]        self.args = args        self.total_len = len(self.nlp_dataset)        if self.args.typo_augment:            transformation = CompositeTransformation([                WordSwapRandomCharacterDeletion(),                WordSwapNeighboringCharacterSwap(),                WordSwapRandomCharacterInsertion(),                WordSwapRandomCharacterSubstitution(),                FixWordSwapQWERTY(),            ])            constraints = [MinWordLength(3), StopwordModification(STOPWORDS)]            self.augmenter = Augmenter(transformation=transformation, constraints=constraints, pct_words_to_swap=0)    def create_one_example(self, text_encoding: List[int], is_query=False):        if is_query and self.args.typo_augment and random.random() < 0.5:            # lazy code for now, don't laugh!            original_query = self.tok.decode(text_encoding)            typo_query = self.augmenter.augment(original_query)[0]            item = self.tok.encode_plus(                typo_query,                truncation='only_first',                return_attention_mask=False,                max_length=self.args.q_max_len if is_query else self.args.p_max_len,            )            return item        item = self.tok.encode_plus(            text_encoding,            truncation='only_first',            return_attention_mask=False,            max_length=self.args.q_max_len if is_query else self.args.p_max_len,        )        return item    def __len__(self):        return self.total_len    def __getitem__(self, item) -> [BatchEncoding, List[BatchEncoding]]:        group = self.nlp_dataset[item]        group_batch = []        qid, qry = (group['qry'][k] for k in self.query_columns)        encoded_query = self.create_one_example(qry, is_query=True)        _, pos_psg = [            random.choice(group['pos'])[k] for k in self.document_columns]        group_batch.append(self.create_one_example(pos_psg))        if len(group['neg']) < self.args.train_group_size - 1:            negs = random.choices(group['neg'], k=self.args.train_group_size - 1)        else:            negs = random.sample(group['neg'], k=self.args.train_group_size - 1)        for neg_entry in negs:            _, neg_psg = [neg_entry[k] for k in self.document_columns]            group_batch.append(self.create_one_example(neg_psg))        return encoded_query, group_batchclass GroupedMarcoTrainDatasetV2(GroupedMarcoTrainDataset):    def create_one_example(self, text_encoding: List[int], is_query=False):        sep_pos = text_encoding.index(102)        original_ids = text_encoding[:sep_pos]        exp_ids = text_encoding[sep_pos+1:]        max_length = self.args.q_max_len if is_query else self.args.p_max_len        truncate_stratagy = 'only_second'        if len(text_encoding) + 2 > max_length:            num_truncate = len(text_encoding) + 2 - max_length            if len(exp_ids) <= num_truncate:                exp_ids = [exp_ids[0]]  # keep one expanded for now                truncate_stratagy = 'only_first'        item = self.tok.encode_plus(            original_ids,            exp_ids,            truncation=truncate_stratagy,            return_attention_mask=False,            max_length=self.args.q_max_len if is_query else self.args.p_max_len,        )        if is_query:            item["input_ids"][0] = 2        else:            item["input_ids"][0] = 1        return itemclass MarcoPredDataset(Dataset):    columns = [        'qid', 'pid', 'qry', 'psg'    ]    def __init__(self, path_to_json: List[str], tokenizer: PreTrainedTokenizer, cache_dir: str, q_max_len=16, p_max_len=128):        print(path_to_json, flush=True)        self.nlp_dataset = datasets.load_dataset(            'json',            data_files=path_to_json,            cache_dir=cache_dir,        )['train']        self.tok = tokenizer        self.q_max_len = q_max_len        self.p_max_len = p_max_len        print('dataset loaded', flush=True)    def __len__(self):        return len(self.nlp_dataset)    def __getitem__(self, item) -> [BatchEncoding, BatchEncoding]:        qid, pid, qry, psg = (self.nlp_dataset[item][f] for f in self.columns)        encoded_qry = self.tok.encode_plus(            qry,            truncation='only_first',            return_attention_mask=False,            max_length=self.q_max_len,        )        encoded_psg = self.tok.encode_plus(            psg,            max_length=self.p_max_len,            truncation='only_first',            return_attention_mask=False,        )        return encoded_qry, encoded_psgclass MarcoEncodeDataset(Dataset):    columns = ['pid', 'psg']    def __init__(self, path_to_json: List[str], tokenizer: PreTrainedTokenizer, cache_dir: str, p_max_len=128):        self.nlp_dataset = datasets.load_dataset(            'json',            data_files=path_to_json,            cache_dir=cache_dir,        )['train']        self.tok = tokenizer        self.p_max_len = p_max_len    def __len__(self):        return len(self.nlp_dataset)    def __getitem__(self, item) -> [BatchEncoding, BatchEncoding]:        pid, psg = (self.nlp_dataset[item][f] for f in self.columns)        encoded_psg = self.tok.encode_plus(            psg,            max_length=self.p_max_len,            truncation='only_first',            return_attention_mask=False,        )        return encoded_psgclass MarcoEncodeDatasetV2(MarcoEncodeDataset):    def __getitem__(self, item) -> [BatchEncoding, BatchEncoding]:        pid, psg = (self.nlp_dataset[item][f] for f in self.columns)        sep_pos = psg.index(102)        original_ids = psg[:sep_pos]        exp_ids = psg[sep_pos + 1:]        truncate_stratagy = 'only_second'        if len(psg) + 2 > self.p_max_len:            num_truncate = len(psg) + 2 - self.p_max_len            if len(exp_ids) <= num_truncate:                exp_ids = [exp_ids[0]]  # keep one expanded for now                truncate_stratagy = 'only_first'        encoded_psg = self.tok.encode_plus(            original_ids,            exp_ids,            max_length=self.p_max_len,            truncation=truncate_stratagy,            return_attention_mask=False,        )        encoded_psg["input_ids"][0] = 1        return encoded_psg